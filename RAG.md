# 大模型RAG调研：

## 1、解决什么问题：大模型RAG能够解决哪些非RAG场景不能解决的问题，有哪些典型应用场景。这些场景中大模型RAG是否是必须的，其他模型方案能否替代

大模型RAG能够解决哪些非RAG场景不能解决的问题:

- 处理特定领域或高度专业化的查询方面
    
- 不正确的信息或“幻觉”，特别是当查询超出模型的训练数据或需要最新信息时
    
- 不透明、不可追踪的推理过程
    

应用场景：需要知识更新，外部知识，可解释性，降低幻觉场景

这些场景FT无法替代，因为要增加数据重新训练

## 2、主流结构式什么：大模型RAG主流结构是什么，结构上有哪些常见变体。整体结构中最重要的是哪几个模块。目前瓶颈在哪里，未来会有哪些优化方向

主流结构

RAG 工作流程包括三个关键步骤。首先，将语料库划分为离散块，然后利用编码器模型构建向量索引。其次，RAG 根据块与查询和索引块的向量相似性来识别和检索块。最后，该模型根据从检索到的块中收集的上下文信息合成一个响应。

Naive RAG

结构

1. 索引
    
    - Data indexing：清洗和提取原始数据为标准化纯文本
        
    
    - Chunking： 把文本分成更小的块来fit上下文限制
        
    - Embedding and creating Index：通过embedding模型将文本变成向量，建立索引，将这些文本块及其向量嵌入存储为键值对方便查询
        
2. 检索
    
    - 用一样的编码模型将input变成向量，计算query和索引库中vectorized chunks的相似度，得到Top k个最相似的chunks。这些选出来的chunks作为当前查询的补充的上下文
        
3. 回答生成
    
    - 把chunks加入到prompt中，让llm生成回复。模型的回答方法可能会根据特定任务的标准而有所不同，从而允许它利用其固有的参数知识或限制其对所提供文档中包含的信息的响应。
        

Naive RAG 瓶颈或难点

1. 检索质量：
    
    - 精度低，导致检索到的块没有对齐以及幻觉等潜在问题。
        
    - 低召回率，导致无法检索所有相关块，从而阻碍LLMs生成全面答复的能力。
        
    - 过时的信息，导致产生不准确的检索结果。
        
2. 响应生成质量：
    
    - 幻觉，产生的答案不按照提供的上下文
        
    - 模型的毒性或偏见
        
3. 难以高效整合检索到的内容进生成任务：
    
    - 导致脱节或不连贯的输出
        
    - 检索到的内容可能含有相似信息，道中模型重复回答
        
4. 难以判断检索到的段落的重要性与相关性
    
5. 需要调和不同文章的写作方式和语气
    
6. 过度依赖检索到的信息
    
    - 只是重复检索到的信息，不提供新的价值或综合信息。
        

Advanced RAG

针对Naive RAG的缺点做了一些针对性的增强：

1. pre-retrieval：（优化数据索引）
    
    - 增强数据粒度：删除不相关的信息，消除实体和术语中的歧义，确认事实准确性，维护上下文并更新过时的文档
        
    - 优化索引结构：调整块的大小以捕获相关上下文，跨多个索引路径进行查询，并通过利用图数据索引中节点之间的关系合并来自图结构的信息以捕获相关上下文
        
    - 添加元数据：将引用的元数据（例如日期和目的）集成到块中以用于过滤目的，以及合并引用的章节和小节等元数据以提高检索效率。
        
    - 对齐优化：引入假设问题到文章中来对齐
        
    - 混合检索
        
2. retrieval：（找出合适的上下文）
    
    - 微调Embedding模型：定制嵌入模型以增强特定领域上下文中的检索相关性，如BGE embedding model
        
    - 动态嵌入：区别于一个词用固定的一个向量，根据上下文确定一个词的向量
        
    
3. post-retrieval（解决上下文限制）
    
    - Re-Ranking：重排序检索到的内容防止prompt的首部和尾部
        
    - Prompt压缩：压缩不相关的上下文，突出显示关键段落，并减少整体上下文长度
        

模块化RAG

区别于传统的Naive RAG框架，更多功能和更灵活，逐渐成为RAG的范式。它集成了各种方法来增强功能模块。

新模块

1. 搜索模块：与相似性检索不同，搜索模块针对具体场景并且整合了直接的对额外语料的搜索（利用LLM生成code、查询语言、或tool）。
    
2. 记忆模块：利用LLM的记忆能力来指导检索，识别与当前输入最相似的记忆。通过使用自己的输出来改进自身，文本在推理过程中变得更加符合数据分布。
    
3. 融合模块：通过一种多查询方法，使用LLMs将用户查询扩展到多个、不同的视角。确保搜索结果与用户的显式和隐式意图紧密结合。
    
4. 路由模块：决定用户查询的后续操作，选项包括总结概况、搜索特定数据库或将不同路径合并为单个回复。查询路由模块的决策是通过 LLM 调用预定义和执行的，该调用将查询定向到所选索引。
    
5. 额外生成模块：用LLM来产生必要的上下文，LLM产生的比直接检索出的更可能含有相关信息
    
6. 任务适应性模块。该模块侧重于使 RAG 适应各种下游任务。
    

三种RAG关系

Naive是Advanced的特殊形式，Advanced又是Modular的特殊形式

最重要的就是“Retrieve” 和“Read” 模块，Advanced RAG加了“Rewrite” 和“Rerank” 模块，而模块RAG有更多的模块。

目前瓶颈及优化方向

1. 上下文长度。 RAG 的功效受到大型语言模型 (LLM) 上下文窗口大小的限制。平衡窗口太短（可能导致信息不足的风险）和太长（可能导致信息稀释的风险）之间的权衡至关重要。
    
2. 鲁棒性。检索过程中存在噪音或矛盾信息可能会对 RAG 的输出质量产生不利影响。
    
3. 如何把RAG和Fine-tuning结合。是顺序训练、交替训练还是通过端到端联合训练？
    
4. 扩展LLM的角色。除了生成最终答案之外，LLM还可以在 RAG 框架内进行检索和评估。
    
5. RAG的scaling law。
    
6. 如何投入应用。提高检索效率、提高大型知识库中的文档召回率以及确保数据安全（例如防止LLM无意中泄露文档来源或元数据）是仍有待解决的关键工程挑战
    
7. 多模态RAG。图像、音视频、代码。
    

## 3、大模型是否需要针对检索模块进行微调：大模型RAG需不需要对模型进行微调，做zero-shot是否合适

需要对大模型进行微调，zero-shot不合适。

- RAG 的突出之处在于不仅将query，而且将检索器检索到的各种文档（结构化/非结构化）合并到输入中。但这些附加信息可以显着影响模型的理解，特别是对于较小的模型。在这种情况下，微调模型以适应查询和检索文档的输入变得至关重要。
    
- 另外看到有人做了实验，微调gpt3.5-turbo，虽然正确率没有什么改变，但是可以降低幻觉，不会胡编乱造，会说不知道[【OpenAI中文文档】针对RAG的微调 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/670661837)
    
- 0-shot下，提升还是可以的（RAG instruction tuning比zero-shot instruction tuning还好），但5-shot情况下，只调LLM似乎提升不大，可能只带来边际改进，甚至损害模型性能。[RA-DIT](https://arxiv.org/pdf/2310.01352.pdf)
    

另外，检索模块也需要进行微调，单纯FT 检索模块也可以提点。通过各种技术提高检索命中率不一定会改善最终结果，因为检索到的文档可能与LLM没有对齐。另外，微调检索模块能够很好地和当前领域数据对齐。

## Reference
[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)

[RA-DIT](https://arxiv.org/pdf/2310.01352.pdf)
